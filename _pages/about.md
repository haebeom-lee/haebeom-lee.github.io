---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---
I'm an assitant professor in the School of Electrical Engineering at Korea University. Prior to this, I was a postdoctoral researcher at KAIST under the supervision of [Prof. Juho Lee](https://juho-lee.github.io/), and at Mila and Université de Montréal under the supervision of [Prof. Yoshua Bengio](https://yoshuabengio.org/). I earned my Ph.D degree from KAIST under the supervision of [Prof. Sung Ju Hwang](http://www.sungjuhwang.com/).


## Research Interest
My research interest include
- Large Language Model Reasoning
- System 2 Deep Learning
- Meta-Learning / Bi-level Optimization
- AutoML / Hyperparameter Optimization
- Bayesian Inference and Learning
- Generative Flow Networks (GFlowNet)
- Transfer Learning / Multi-Task Learning / Continual Learning

## For the prospective students ##
I am looking for self-motivated students with various backgrounds, like electrical engineering, computer science, or mathematics.
If you are interested in the topics above and doing research together, please feel free to reach out through my email. 
- For the **prospective undergraduate interns**, you don't need to have strong backgrounds on AI or machine learining. How enthusiastic you are is probably the most important.
- For the **prospective master students**, I may expect you to have some backgrounds on AI or machine learning, e.g., having interned at other AI/ML labs or company, or having good grades in AI-related courses.
- For the **prospective PhD students**, I expect you to have published at least one first-authored paper to top-tier machine learning conferences, e.g., NeurIPS, ICML, ICLR, ACL, EMNLP, CVPR, ICCV, etc.

In case of applying for our lab as a graduate student, I strongly recommend you to have an internship period in our lab for at least 3 months, before applying.

## For the students taking Engineering Design I or II ##
Please feel free to reach out if you are interested or if you'd like to have an experience in machine learning or deep learning. I can provide the following for one semester:
- An opportunity to do actual research together on some recent topics in machine learning and deep learning.
- An opportunity to write a research paper.
- Google Colab pro plus account provided for 3 months, for all students.

## Contact
haebeomlee dot korea dot ac dot kr

## Awards
- Global Ph.D Fellowship Program, 2019-2021
- Google Ph.D Fellowship Program 2021
- Outstanding reviewer (ICML2020 - Top 33%, ICML2022 - Top 10%)


## New Preprints

- <font size="4">Dataset Condensation with Latent Space Knowledge Factorization and Sharing</font>
[[paper]](http://arxiv.org/abs/2208.10494) \\
**Hae Beom Lee\***, Dong Bok Lee\*, Sung Ju Hwang \\
(\*: equal contribution) \\
arXiv, 2022

- <font size="4">Cost-Sensitive Multi-Fidelity Bayesian Optimization with Transfer of Learning Curve Extrapolation</font>
[[paper]](https://arxiv.org/abs/2405.17918) \\
Dong Bok Lee, Aoxuan Silvia Zhang\*, Byungjoo Kim\*, Junhyeon Park\*, Juho Lee, Sung Ju Hwang, **Hae Beom Lee** \\
(\*: equal contribution) \\
arXiv, 2024

## Conference Publications

- <font size="4">Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks</font>
Dongwoo Lee\*, Dong Bok Lee\*, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Frank Hutter, Seon Joo Kim,  **Hae Beom Lee** \\
(\*: equal contribution) \\
<span style="color:darkred">**ICML**</span> 2025

- <font size="4">Delta-AI: Local Objectives for Amortized Inference in Sparse Graphical Models</font>
[[paper]](https://arxiv.org/abs/2310.02423) \\
Jean-Pierre René Falet*, **Hae Beom Lee\***, Nikolay Malkin\*, Chen Sun, Dragos Secrieru, Dinghuai Zhang, Guillaume Lajoie, Yoshua Bengio \\
(\*: equal contribution) \\
<span style="color:darkred">**ICLR**</span> 2024

- <font size="4">Online Hyperparameter Meta-Learning with Hypergradient Distillation</font>
[[paper]](http://arxiv.org/abs/2110.02508) \\
 **Hae Beom Lee**, Hayeon Lee, Jaewoong Shin, Eunho Yang, Timothy M. Hospedales, Sung Ju Hwang \\
<span style="color:darkred">**ICLR**</span> 2022 <span style="color:darkred">**(spotlight)**</span>

- <font size="4">Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning</font>
[[paper]](https://arxiv.org/abs/2110.02600) \\
Seanie Lee\*, **Hae Beom Lee\***, Juho Lee, Sung Ju Hwang \\
(\*: equal contribution) \\
<span style="color:darkred">**ICLR**</span> 2022

- <font size="4">Meta-Learning Low Rank Covariance Factors for Energy-Based Deterministic Uncertainty</font>
[[paper]](https://arxiv.org/abs/2110.06381) \\
Jeffrey Ryan Willette, **Hae Beom Lee**, Juho Lee, Sung Ju Hwang \\
<span style="color:darkred">**ICLR**</span> 2022

- <font size="4">Large-Scale Meta-Learning with Continual Trajectory Shifting</font>
[[paper]](https://arxiv.org/pdf/2102.07215.pdf) [[code]](https://github.com/JWoong148/ContinualTrajectoryShifting) \\
Jaewoong Shin\*, **Hae Beom Lee\***, Boqing Gong, Sung Ju Hwang \\
(\*: equal contribution) \\
<span style="color:darkred">**ICML**</span> 2021

- <font size="4">MetaPerturb: Transferable Regularizer for Heterogeneous Tasks and Architectures</font>
[[paper]](https://papers.nips.cc/paper/2020/file/84ddfb34126fc3a48ee38d7044e87276-Paper.pdf) [[code]](https://github.com/JWoong148/metaperturb) \\
Jeongun Ryu\*, Jaewoong Shin\*, **Hae Beom Lee\***, Sung Ju Hwang \\
(\*: equal contribution) \\
<span style="color:darkred">**NeurIPS**</span> 2020 <span style="color:darkred">**(spotlight)**</span>

- <font size="4">Meta-Learning for Short Utterance Speaker Recognition with Imbalance Length Pairs</font>
[[paper]](https://arxiv.org/pdf/2004.02863.pdf) [[code]](https://github.com/seongmin-kye/meta-SR) \\
Seong Min Kye, Youngmoon Jung, **Hae Beom Lee**, Sung Ju Hwang, and Hoirin Kim \\
<span style="color:darkred">**Interspeech**</span> 2020

- <font size="4">Meta Variance Transfer: Learning to Augment from the Others</font>
[[paper]](https://proceedings.icml.cc/static/paper_files/icml/2020/2222-Paper.pdf) \\
Seong Jin Park, Seungju Han, Ji-won Baek, Insoo Kim, Juhwan Song, **Hae Beom Lee**, Jae-Joon Han and Sung Ju Hwang \\
<span style="color:darkred">**ICML**</span> 2020

- <font size="4">Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks</font>
[[paper]](https://openreview.net/pdf?id=rkeZIJBYvr) [[code]](https://github.com/haebeom-lee/l2b) \\
**Hae Beom Lee\***, Hayeon Lee\*, Donghyun Na\*, Saehoon Kim, Minseop Park, Eunho Yang, Sung Ju Hwang \\
(\*: equal contribution) \\
<span style="color:darkred">**ICLR**</span> 2020 <span style="color:darkred">**(oral presentation)**</span>

- <font size="4">Meta Dropout: Learning to Perturb Latent Features for Generalization</font>
[[paper]](https://openreview.net/pdf?id=BJgd81SYwr) [[code]](https://github.com/haebeom-lee/metadrop) \\
**Hae Beom Lee**, Taewook Nam, Eunho Yang, Sung Ju Hwang \\
<span style="color:darkred">**ICLR**</span> 2020

- <font size="4">DropMax: Adaptive Variational Softmax</font>
[[paper]](https://arxiv.org/pdf/1712.07834.pdf)[[code]](https://github.com/haebeom-lee/dropmax) \\
**Hae Beom Lee**, Juho Lee, Saehoon Kim, Eunho Yang, Sung Ju Hwang \\
<span style="color:darkred">**NeurIPS**</span> 2018

- <font size="4">Uncertainty-Aware Attention for Reliable Interpretation and Prediction</font>
[[paper]](https://arxiv.org/pdf/1805.09653.pdf)[[code]](https://github.com/jayheo/UA) \\
Jay Heo\*, **Hae Beom Lee\***, Saehoon Kim, Juho Lee, Kwang Joon Kim, Eunho Yang, Sung Ju Hwang \\
(\*: equal contribution) \\
<span style="color:darkred">**NeurIPS**</span> 2018

- <font size="4">Deep Asymmetric Multi-task Feature Learning</font>
[[paper]](https://arxiv.org/pdf/1708.00260.pdf)[[code]](https://github.com/haebeom-lee/amtfl) \\
**Hae Beom Lee**, Eunho Yang, Sung Ju Hwang \\
<span style="color:darkred">**ICML**</span> 2018

## Old Preprints

- <font size="4">Meta-Learned Confidence for Few-shot Learning</font>
[[paper]](https://arxiv.org/pdf/2002.12017.pdf)[[code]](https://github.com/seongmin-kye/MCT) \\
Sung Min Kye, **Hae Beom Lee**, Hoirin Kim, Sung Ju Hwang \\
arXiv, 2020

- <font size="4">Adaptive Network Sparsification with Dependent Variational Beta-Bernoulli Dropout</font>
[[paper]](https://arxiv.org/pdf/1805.10896.pdf)[[code]](https://github.com/OpenXAIProject/Variational_Dropouts) \\
Juho Lee, Saehoon Kim, Jaehong Yoon, **Hae Beom Lee**, Eunho Yang, Sung Ju Hwang \\
arXiv, 2018


